{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "import joblib as joblib\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# pip install unidecode\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_train = pd.read_csv('../../Data Splits/train_data_70_30.csv')\n",
    "data_val = pd.read_csv('../../Data Splits/val_data_70_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['article']\n",
    "y_train = data_train['label']\n",
    "\n",
    "X_val = data_val['article']\n",
    "y_val = data_val['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword Removal\n",
    "ps = PorterStemmer()\n",
    "with open('../../Datasets/stopwords-tl.json', 'r') as f:\n",
    "    stopwords = json.load(f)\n",
    "\n",
    "# Custom transformer for text preprocessing\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [' '.join(self.preprocess(text)) for text in X]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        # Lowercase Conversion\n",
    "        lowered = text.lower()\n",
    "\n",
    "        # URL Removal\n",
    "        urled = re.sub(r'https?://\\S+|www\\.\\S+', '', lowered)\n",
    "        \n",
    "        # Text Simplification\n",
    "        text = re.sub(r'\\[.*?\\]', '', urled)\n",
    "        text = re.sub(r\"\\\\W\", \" \", text)\n",
    "        text = re.sub(r'<.*?>+', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub(r'\\n', '', text)\n",
    "        text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stems = [ps.stem(token) for token in tokens]\n",
    "        filtered = [stem for stem in stems if stem not in stopwords]\n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing base models\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "svm = SVC(probability=True)\n",
    "\n",
    "# mnb = MultinomialNB(alpha= 0.1, fit_prior=False)\n",
    "# lr = LogisticRegression(C= 100, penalty= 'l2', solver= 'liblinear')\n",
    "# rf = RandomForestClassifier(n_estimators= 300, random_state=42)\n",
    "# knn = KNeighborsClassifier(metric= 'euclidean', n_neighbors= 5, weights= 'uniform')\n",
    "# svm = SVC(C=10, degree=2, kernel = 'linear', probability = True)\n",
    "\n",
    "base_models = [mnb, lr, rf, knn, svm]\n",
    "base_names = ['MNB', 'LR', 'RF', 'KNN', 'SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_rf = joblib.load('../../ModelsV2/pipeline_rf_964.joblib')\n",
    "stack_lr = joblib.load('../../ModelsV2/pipeline_lr.joblib')\n",
    "stack_svm = joblib.load('../../ModelsV2/pipeline_svm_lendon.joblib')\n",
    "stack_mlp = joblib.load('../../ModelsV2/pipeline_mlp.joblib')\n",
    "\n",
    "models = [\n",
    "    (stack_lr, 'Logistic Regression'),\n",
    "    (stack_rf, 'Random Forest'),\n",
    "    (stack_svm, 'Support Vector Machine'),\n",
    "    (stack_mlp, 'Multi-layer Perceptron'),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Models: (TRAINING)\n",
      "                        Accuracy Precision    Recall  F1 Score       AUC\n",
      "MNB                      0.92019   0.86685  0.993684  0.925944  0.995083\n",
      "LR                      0.952431  0.986425  0.917895  0.950927  0.996231\n",
      "RF                      0.985201       1.0  0.970526  0.985043   0.99965\n",
      "KNN                     0.901163  0.855545  0.966316  0.907563  0.977569\n",
      "SVM                     0.996829       1.0  0.993684  0.996832       1.0\n",
      "Stacking Model                                                          \n",
      "Logistic Regression          1.0       1.0       1.0       1.0       1.0\n",
      "Random Forest                1.0       1.0       1.0       1.0       1.0\n",
      "Support Vector Machine       1.0       1.0       1.0       1.0       1.0\n",
      "Multi-layer Perceptron       1.0       1.0       1.0       1.0       1.0\n"
     ]
    }
   ],
   "source": [
    "base_names = ['MNB', 'LR', 'RF', 'KNN', 'SVM']\n",
    "metrics_df = pd.DataFrame(index=base_names, columns=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'])\n",
    "\n",
    "vectorized_text = stack_rf.named_steps['vectorizer'].transform(X_train)\n",
    "\n",
    "# Evaluate base models\n",
    "for model, name in  zip(stack_rf.named_steps['stacking'].estimators_ , base_names):\n",
    "    y_pred = model.predict(vectorized_text)\n",
    "    y_prob = model.predict_proba(vectorized_text)[:, 1]  # Probability for positive class for ROC curve\n",
    "    \n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    precision = precision_score(y_train, y_pred)\n",
    "    recall = recall_score(y_train, y_pred)\n",
    "    f1 = f1_score(y_train, y_pred)\n",
    "    auc = roc_auc_score(y_train, y_prob)\n",
    "\n",
    "    # Store metrics in the DataFrame\n",
    "    metrics_df.loc[name] = [accuracy, precision, recall, f1, auc]\n",
    "\n",
    "\n",
    "# Evaluate stacking models\n",
    "def get_performance_stats(model, name):\n",
    "    stack_y_pred = model.predict(X_train)\n",
    "    stack_y_prob = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    stack_accuracy = accuracy_score(y_train, stack_y_pred)\n",
    "    stack_precision = precision_score(y_train, stack_y_pred)\n",
    "    stack_recall = recall_score(y_train, stack_y_pred)\n",
    "    stack_f1 = f1_score(y_train, stack_y_pred)\n",
    "    stack_auc = roc_auc_score(y_train, stack_y_prob)\n",
    "    metrics_df.loc[name] = [stack_accuracy, stack_precision, stack_recall, stack_f1, stack_auc]\n",
    "\n",
    "metrics_df.loc['Stacking Model'] = [\"\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "for model, name in models:\n",
    "    get_performance_stats(model, name)\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"Comparison of Models: (TRAINING)\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = pd.read_csv('../../Data Splits/test_data.csv')\n",
    "\n",
    "training_x = training['article']\n",
    "\n",
    "training_y = training['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Models: (TESTING)\n",
      "                        Accuracy Precision    Recall  F1 Score       AUC\n",
      "MNB                     0.813953   0.73399  0.986755  0.841808  0.959647\n",
      "LR                      0.877076  0.945312  0.801325  0.867384  0.968433\n",
      "RF                      0.893688  0.947368  0.834437  0.887324  0.970044\n",
      "KNN                     0.827243  0.779661  0.913907  0.841463  0.922097\n",
      "SVM                     0.893688     0.976  0.807947  0.884058   0.97351\n",
      "Stacking Model                                                          \n",
      "Logistic Regression     0.940199  0.946309  0.933775      0.94   0.98287\n",
      "Random Forest           0.943522  0.935065  0.953642  0.944262  0.980795\n",
      "Support Vector Machine  0.946844   0.94702   0.94702   0.94702  0.983841\n",
      "Multi-layer Perceptron  0.940199  0.946309  0.933775      0.94  0.982296\n"
     ]
    }
   ],
   "source": [
    "base_names = ['MNB', 'LR', 'RF', 'KNN', 'SVM']\n",
    "metrics_df = pd.DataFrame(index=base_names, columns=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'])\n",
    "\n",
    "vectorized_text = stack_rf.named_steps['vectorizer'].transform(training_x)\n",
    "\n",
    "# Evaluate base models\n",
    "for model, name in  zip(stack_rf.named_steps['stacking'].estimators_ , base_names):\n",
    "    y_pred = model.predict(vectorized_text)\n",
    "    y_prob = model.predict_proba(vectorized_text)[:, 1]  # Probability for positive class for ROC curve\n",
    "    \n",
    "    accuracy = accuracy_score(training_y, y_pred)\n",
    "    precision = precision_score(training_y, y_pred)\n",
    "    recall = recall_score(training_y, y_pred)\n",
    "    f1 = f1_score(training_y, y_pred)\n",
    "    auc = roc_auc_score(training_y, y_prob)\n",
    "    metrics_df.loc[name] = [accuracy, precision, recall, f1, auc]\n",
    "\n",
    "\n",
    "# Evaluate stacking models\n",
    "def get_performance_stats(model, name):\n",
    "    stack_y_pred = model.predict(training_x)\n",
    "    stack_y_prob = model.predict_proba(training_x)[:, 1]  \n",
    "\n",
    "    stack_accuracy = accuracy_score(training_y, stack_y_pred)\n",
    "    stack_precision = precision_score(training_y, stack_y_pred)\n",
    "    stack_recall = recall_score(training_y, stack_y_pred)\n",
    "    stack_f1 = f1_score(training_y, stack_y_pred)\n",
    "    stack_auc = roc_auc_score(training_y, stack_y_prob)\n",
    "    metrics_df.loc[name] = [stack_accuracy, stack_precision, stack_recall, stack_f1, stack_auc]\n",
    "\n",
    "metrics_df.loc['Stacking Model'] = [\"\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "for model, name in models:\n",
    "    get_performance_stats(model, name)\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"Comparison of Models: (TESTING)\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Models: (VALIDATION)\n",
      "                        Accuracy Precision    Recall  F1 Score       AUC\n",
      "MNB                     0.820197  0.741697  0.985294  0.846316  0.975102\n",
      "LR                      0.917488   0.96206  0.870098  0.913771  0.979312\n",
      "RF                      0.919951  0.957333  0.879902  0.916986  0.975396\n",
      "KNN                     0.862069  0.816239  0.936275  0.872146  0.934922\n",
      "SVM                     0.919951  0.967302  0.870098  0.916129  0.983686\n",
      "Stacking Model                                                          \n",
      "Logistic Regression     0.955665  0.958128  0.953431  0.955774  0.993272\n",
      "Random Forest           0.964286  0.961071  0.968137  0.964591  0.994437\n",
      "Support Vector Machine  0.950739  0.953202  0.948529   0.95086  0.991294\n",
      "Multi-layer Perceptron  0.949507   0.95086  0.948529  0.949693  0.992501\n"
     ]
    }
   ],
   "source": [
    "base_names = ['MNB', 'LR', 'RF', 'KNN', 'SVM']\n",
    "metrics_df = pd.DataFrame(index=base_names, columns=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'])\n",
    "\n",
    "vectorized_text = stack_rf.named_steps['vectorizer'].transform(X_val)\n",
    "\n",
    "# Evaluate base models\n",
    "for model, name in  zip(stack_rf.named_steps['stacking'].estimators_ , base_names):\n",
    "    y_pred = model.predict(vectorized_text)\n",
    "    y_prob = model.predict_proba(vectorized_text)[:, 1]  # Probability for positive class for ROC curve\n",
    "    \n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_prob)\n",
    "\n",
    "    # Store metrics in the DataFrame\n",
    "    metrics_df.loc[name] = [accuracy, precision, recall, f1, auc]\n",
    "\n",
    "# Evaluate stacking models\n",
    "def get_performance_stats(model, name):\n",
    "    stack_y_pred = model.predict(X_val)\n",
    "    stack_y_prob = model.predict_proba(X_val)[:, 1]  \n",
    "\n",
    "    stack_accuracy = accuracy_score(y_val, stack_y_pred)\n",
    "    stack_precision = precision_score(y_val, stack_y_pred)\n",
    "    stack_recall = recall_score(y_val, stack_y_pred)\n",
    "    stack_f1 = f1_score(y_val, stack_y_pred)\n",
    "    stack_auc = roc_auc_score(y_val, stack_y_prob)\n",
    "    metrics_df.loc[name] = [stack_accuracy, stack_precision, stack_recall, stack_f1, stack_auc]\n",
    "\n",
    "metrics_df.loc['Stacking Model'] = [\"\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "for model, name in models:\n",
    "    get_performance_stats(model, name)\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"Comparison of Models: (VALIDATION)\")\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
